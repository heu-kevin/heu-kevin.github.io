<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/03/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>hello-blog</title>
    <url>/2021/03/21/hello-blog/</url>
    <content><![CDATA[<p>打今儿起，我也终于有自己的博客啦！</p>
]]></content>
      <categories>
        <category>啥也不是</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title>个性化联邦学习：Personalized Federated Learning</title>
    <url>/2021/03/24/%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9APersonalized-Federated-Learning/</url>
    <content><![CDATA[<p>今日调研了Alireza Fallah等人提出的个性化联邦学习，<a href="https://arxiv.org/abs/2002.07948">论文链接</a></p>
<span id="more"></span>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在联邦学习场景中，存在$n$个参与方和一个服务器，其中每个参与方都只能访问自己本地的数据样本，利用本地数据样本训练本地模型，再将本地模型参数上传至服务器，服务器收到所有进行本地训练的参与方的本地模型参数后，对其进行聚合（往往是加权平均）获得全局模型参数，再将其广播给所有的参与方，进行下一轮训练。将$f_i:R^d\rightarrow R$作为参与方$i$的损失函数，那么联邦学习的目标就是：</p>
<p>​                                                                              ${\underset {w\in R^d}{min}}f(w):=\frac{1}{n}\sum_{i=1}^nf_i(w)$           (1)</p>
<p>对于监督学习，$f_i$可以代表参与方$i$的本地数据分布上的期望损失：</p>
<p>​                                                                              $f_i(w):=E_{(x,y)\sim p_i}[l_i(w;x,y)]$        (2)</p>
<p>其中$l_i(w;x,y)$代表模型$w$在预测标签为$y$的样本$x$时造成的损失，$p_i$是用户$i$的数据分布。在数据异构性强的设定下不同参与方的本地数据的分布是互不相同的。那么$f_i(w)$同样可以写作$f_i(w)=\sum_{(x,y)\in S_i}p_i(x,y)l_i(w;x,y)$，其中$S_i$代表参与方$i$的本地数据集，$p_i(x,y)$代表该样本在参与方$i$的本地数据集被采样的概率。</p>
<p>这样子最后获得的全局模型并不能适用于每个用户，特别是在用户的底层数据分布不相同的异构设置中，通过最小化平均损失获得的全局模型一旦应用到每个用户的本地数据集上，可能会执行得很糟糕。换句话说，式（1）给出的解决方案并不是针对每个用户的个性化，不能完全实现特定于用户的模型。</p>
<p>故而本文作者提出了个性化联邦学习，其目标是找到一个所有参与方共享的初始点，每个参与方依据其自身的损失函数更新这个初始点，执行比较少的梯度更新步骤就可以表现良好。这样，虽然初始模型在所有参与方中一致，但每个参与方实现的最终模型根据其本地数据的不同而不同。作者研究了FedAvg算法的变体——Per-FedAvg，用于解决个性化联邦学习问题。</p>
<h2 id="通过MAML进行个性化联邦学习"><a href="#通过MAML进行个性化联邦学习" class="headerlink" title="通过MAML进行个性化联邦学习"></a>通过MAML进行个性化联邦学习</h2><p>首先回顾MAML。与传统的监督学习设置不同，在MAML中，给定一组从基础分布中抽取的任务，目标不是找到一个在预期的所有任务上都表现良好的模型，而是找到一个初始化模型，它基于新的任务在经过较少的训练更新步骤之后，在该任务上表现良好。特别地，作者假设联邦学习中每个参与方都取这个初始点，并根据自身的损失函数更新一次模型，那么式（1）中的问题就变成了：</p>
<p>​                                                                      ${\underset {w\in R^d}{min}}F(w):=\frac{1}{n}\sum_{i=1}^nf_i(w-\alpha \nabla f_i(w))$       (3)</p>
<p>这样不仅保持了联邦学习的优点，也捕获了不同参与方之间的区别。这样式（3）求得的解决方案就可以作为初始模型分发给各个参与方，各个参与方在此基础上依据本地数据进行较少的训练更新步骤就可以获取一个适合自己的数据集的模型。</p>
<h2 id="个性化联邦学习算法：Per-Fedavg"><a href="#个性化联邦学习算法：Per-Fedavg" class="headerlink" title="个性化联邦学习算法：Per-Fedavg"></a>个性化联邦学习算法：Per-Fedavg</h2><p>式（3）中的$F$可以写作元函数$F_1,F_2,…,F_n$的平均，其中元函数$F_i$代表参与方$i$的元函数：</p>
<p>​                                                                  $F_i(w):=f_i(w-\alpha \nabla f_i(w))$              (4)</p>
<p>类似于FedAvg，第一步是各个参与方计算本地元函数的梯度：</p>
<p>​                                                                $\nabla F_i(w)=(I-\alpha \nabla ^2f_i(w))\nabla f_i(w-\alpha \nabla f_i(w))$        （5）</p>
<p>但是每一轮都计算$\nabla f_i(w)$代价很高，这里取一批关于分布$p_i$的数据$D^i$，得到一个无偏估计：</p>
<p>​                                                                $\widetilde \nabla f_i(w,D^i):=\frac{1}{|D^i|}\sum_{(x,y)\in D^i}\nabla l_i(w;x,y)$           （6）</p>
<p>同理，式（5）中的$\nabla ^2f_i(w)$可以用$\widetilde \nabla ^2f_i(w,D^i)$代替。</p>
<p>在Per-FedAvg的第k轮，类似于FedAVg，服务器将当前的全局模型发送给随机选择的一批参与方（也可以是全部参与方）$A_k$，每一个被选中的参与方都要在本地执行$\tau$个更新步骤，这样每个参与方都会生产一个本地模型参数序列$\{w^i_{k+1,t}\}^\tau _{t=0}$，其中$w_{k+1,0}^i=w_k$，并且满足$\tau \geq t\geq 1$  ：</p>
<p>​                                                                        $w_{k+1,t}^i=w_{k+1,t-1}^i-\beta \widetilde{\nabla}F_i(w_{k+1,t-1}^i)$        （7）</p>
<p>注意，所有局部迭代的$\widetilde{\nabla}F_i(w_{k+1,t-1}^i)$是使用互相独立的批次$D_t^i,D_t^{\prime i},D_t^{\prime \prime i}$计算的：</p>
<img src="/2021/03/24/%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9APersonalized-Federated-Learning/1.png" class="">
<p>然后被选中的参与方将结果发送给服务器，服务器对收到的模型进行平均，获得全局模型$w_{k+1}=\frac{1}{|A_k|}\sum_{i\in A_k}w_{k+1,\tau}^i$。Per-FedAVg算法的伪代码如下图：</p>
<img src="/2021/03/24/%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9APersonalized-Federated-Learning/2.png" class="">
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分就不再赘述了，具体细节见论文原文。这里贴上实验结果。</p>
<img src="/2021/03/24/%E4%B8%AA%E6%80%A7%E5%8C%96%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9APersonalized-Federated-Learning/3.png" class="">
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇论文的作者提出的个性化联邦学习很有想法，值得我们学习，毕竟联邦学习大多数时候各个参与方的本地数据分布就是差异性极大，个性化联邦学习专门迎合这个问题，并尝试解决它。</p>
]]></content>
      <categories>
        <category>联邦学习</category>
      </categories>
      <tags>
        <tag>联邦学习</tag>
        <tag>元学习</tag>
      </tags>
  </entry>
  <entry>
    <title>小小酥的每日学习小结（1）</title>
    <url>/2021/04/04/%E5%B0%8F%E5%B0%8F%E9%85%A5%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93%EF%BC%881%EF%BC%89/</url>
    <content><![CDATA[<p>小小酥备战事业单位考试的学习总结第一天。</p>
<span id="more"></span>
<h1 id="每日学习小结"><a href="#每日学习小结" class="headerlink" title="每日学习小结"></a>每日学习小结</h1><h2 id="1-党建"><a href="#1-党建" class="headerlink" title="1.党建"></a>1.党建</h2><ol>
<li><p>党的性质、指导思想、宗旨</p>
</li>
<li><p>党的纲领&amp;路线</p>
</li>
<li><p>党的思想、组织、制度、作风、反腐倡廉</p>
</li>
<li><p>党的纪律&amp;统一团结</p>
</li>
<li><p>党&amp;党员</p>
</li>
</ol>
<h2 id="2-毛思"><a href="#2-毛思" class="headerlink" title="2.毛思"></a>2.毛思</h2><p>1.毛思形成的5个历史条件</p>
<p>2.毛思的四个阶段</p>
<p>每个阶段包含时期、著作、标志</p>
<ul>
<li>萌芽 </li>
<li>形成</li>
<li>成熟</li>
<li>发展</li>
</ul>
<h2 id="3-邓论"><a href="#3-邓论" class="headerlink" title="3.邓论"></a>3.邓论</h2><ol>
<li><p>邓论形成的4个历史条件</p>
</li>
<li><p>邓论形成&amp;发展的4个过程</p>
</li>
</ol>
]]></content>
      <categories>
        <category>小小酥事业单位考试</category>
      </categories>
      <tags>
        <tag>小小酥</tag>
        <tag>每日总结</tag>
      </tags>
  </entry>
  <entry>
    <title>小小酥的每日学习小结（2）</title>
    <url>/2021/04/05/%E5%B0%8F%E5%B0%8F%E9%85%A5%E7%9A%84%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93%EF%BC%882%EF%BC%89/</url>
    <content><![CDATA[<p>今天是小小酥备战事业单位考试的第二天。</p>
<span id="more"></span>
<h1 id="每日学习小结"><a href="#每日学习小结" class="headerlink" title="每日学习小结"></a>每日学习小结</h1><h2 id="1-毛思（下）"><a href="#1-毛思（下）" class="headerlink" title="1.毛思（下）"></a>1.毛思（下）</h2><h3 id="1-毛思科学内涵（3点）"><a href="#1-毛思科学内涵（3点）" class="headerlink" title="1.毛思科学内涵（3点）"></a>1.毛思科学内涵（3点）</h3><h3 id="2-毛思主要内容"><a href="#2-毛思主要内容" class="headerlink" title="2.毛思主要内容"></a>2.毛思主要内容</h3><ul>
<li>新民主主义革命理论</li>
<li>社主革命&amp;建设理论</li>
<li>人民军队</li>
<li>政策&amp;策略</li>
<li>思想政治&amp;文化工作</li>
<li>党的建设</li>
</ul>
<h3 id="3-毛思活的灵魂"><a href="#3-毛思活的灵魂" class="headerlink" title="3.毛思活的灵魂"></a>3.毛思活的灵魂</h3><p>实事求是、群众路线、独立自主</p>
<h2 id="2-邓论-邓论的主要内容"><a href="#2-邓论-邓论的主要内容" class="headerlink" title="2.邓论-邓论的主要内容"></a>2.邓论-邓论的主要内容</h2><h3 id="1-邓论的精髓"><a href="#1-邓论的精髓" class="headerlink" title="1.邓论的精髓"></a>1.邓论的精髓</h3><p>解放思想、实事求是</p>
<h3 id="2-邓论的本质-（important）"><a href="#2-邓论的本质-（important）" class="headerlink" title="2.邓论的本质 （important）"></a>2.邓论的本质 （important）</h3><h4 id="1）本质的形成-（4个阶段）"><a href="#1）本质的形成-（4个阶段）" class="headerlink" title="1）本质的形成 （4个阶段）"></a>1）本质的形成 （4个阶段）</h4><ul>
<li>1980，提出“生产力…一切…的标准“</li>
<li>1986，发展生产&amp;共同致富</li>
<li>1992，正式提出</li>
<li>促进人的全面发展</li>
</ul>
<h4 id="2）本质的内容"><a href="#2）本质的内容" class="headerlink" title="2）本质的内容"></a>2）本质的内容</h4><p>解放&amp;发展生产力，消灭剥削&amp;两级分化，共同富裕</p>
<h3 id="3-邓论的路线-amp-纲领"><a href="#3-邓论的路线-amp-纲领" class="headerlink" title="3.邓论的路线&amp;纲领"></a>3.邓论的路线&amp;纲领</h3><h4 id="1）路线"><a href="#1）路线" class="headerlink" title="1）路线"></a>1）路线</h4><p>团结和领导人民</p>
<p>一个中心两个基本点</p>
<p>自立更生，艰苦创业</p>
<p>富强民主文明和谐美丽的社主现代化强国</p>
<h4 id="2）纲领"><a href="#2）纲领" class="headerlink" title="2）纲领"></a>2）纲领</h4><ul>
<li>政治</li>
<li>经济</li>
<li>文化</li>
</ul>
<h3 id="4-邓论的发展战略-（important）"><a href="#4-邓论的发展战略-（important）" class="headerlink" title="4.邓论的发展战略 （important）"></a>4.邓论的发展战略 （important）</h3><h4 id="1）三步走"><a href="#1）三步走" class="headerlink" title="1）三步走"></a>1）三步走</h4><ol>
<li>十三大</li>
<li>十五大</li>
<li>十七大</li>
<li>十九大</li>
</ol>
<h4 id="2）新型工业化、区域经济、城镇化"><a href="#2）新型工业化、区域经济、城镇化" class="headerlink" title="2）新型工业化、区域经济、城镇化"></a>2）新型工业化、区域经济、城镇化</h4><h4 id="1-新型工业化"><a href="#1-新型工业化" class="headerlink" title="1.新型工业化"></a>1.新型工业化</h4><p>坚持以信息带动工业化，工业化促进信息化</p>
<p>科技、经济、资源、环境、人力</p>
<h4 id="2-区域经济"><a href="#2-区域经济" class="headerlink" title="2.区域经济"></a>2.区域经济</h4><p>西部大开发（12个省，5个自治区）</p>
<h4 id="3）科教兴国-amp-可持续发展战略"><a href="#3）科教兴国-amp-可持续发展战略" class="headerlink" title="3）科教兴国&amp;可持续发展战略"></a>3）科教兴国&amp;可持续发展战略</h4><p>对邓“科学是第一生产力“的实践&amp;贯彻落实</p>
<p>优先教育、关键在人</p>
<p>创新能力：综合国力决定性因素</p>
<p>生产发展、生活富裕、生态良好</p>
<h4 id="4）乡村振兴战略（补充，十九大提出）"><a href="#4）乡村振兴战略（补充，十九大提出）" class="headerlink" title="4）乡村振兴战略（补充，十九大提出）"></a>4）乡村振兴战略（补充，十九大提出）</h4><p>三农问题（农业农村农民）</p>
<h2 id="3-邓论-改革"><a href="#3-邓论-改革" class="headerlink" title="3.邓论-改革"></a>3.邓论-改革</h2><h3 id="1-制度-（革命是制度）"><a href="#1-制度-（革命是制度）" class="headerlink" title="1.制度 （革命是制度）"></a>1.制度 （革命是制度）</h3><ol>
<li>根本制度：社会主义制度</li>
<li>经济 （2）</li>
<li>政治 （一根本，三基本）</li>
<li>文化（null)</li>
</ol>
<h3 id="2-体制-（改革是体制）"><a href="#2-体制-（改革是体制）" class="headerlink" title="2.体制 （改革是体制）"></a>2.体制 （改革是体制）</h3><ol>
<li>经济：市场经济</li>
<li>政治：发展社主，推进政治改革（重要性，目标，内容，原则），依法治国</li>
<li>文化：特点，作用，内容</li>
</ol>
]]></content>
      <categories>
        <category>小小酥事业单位考试</category>
      </categories>
      <tags>
        <tag>小小酥</tag>
        <tag>每日总结</tag>
      </tags>
  </entry>
  <entry>
    <title>集群联邦学习：Clustered Federated Learning</title>
    <url>/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/</url>
    <content><![CDATA[<p>今日调研了Felix Sattler等人提出的集群联邦学习，<a href="https://ieeexplore.ieee.org/abstract/document/9174890">论文链接</a>。</p>
<span id="more"></span>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>联邦学习中假设全局模型可以同时拟合所有参与方的本地数据，但是显然是不可能的，下图中给出反例。</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/1.jpg" class="">
<p><strong>假设1</strong>：存在$\theta^*$在所有参与方的数据分布上都可以最小化损失：</p>
<p>$R_i(\theta^*)\leq {\underset {\theta}{min}}R_i(\theta)$         $i=1,…,M$</p>
<p>这样，如果所有参与方满足假设1，就称所有参与方的本地数据分布是<strong><font color="#ff0000">全等</font></strong>的。但是事实上，假设1在联邦学习中经常被违反，例如下面这四个场景：</p>
<ul>
<li>不同的参与方具有不同的喜好：例如要在很多个用户所拥有的人脸数据集上联邦训练一个“人脸吸引力”分类器，但是不同的用户对人脸的喜好是不一样的，这导致很难满足假设1。</li>
<li>模型复杂度受限制：假设许多用户正视图联邦训练一个语言模型，用于私有文本的下一个单词预测。在这个场景中，客户的文本信息的统计数据会根据人口统计因素、兴趣等发生很大的变化，一个不够复杂的模型不能同时拟合所有客户端的数据。</li>
<li>敌手的存在：如果用户中存在敌手，敌手可以故意改变自己的本地数据分布，对联邦模型造成恶劣的影响。</li>
<li>联邦多任务学习：联邦多任务学习的目标是为每个参与方提供一个最适合其本地数据分布的模型。普通的联邦学习框架下，只有一个单一的全局模型无法实现这一目标。</li>
</ul>
<p>那么，推广一下传统的联邦学习假设呢？</p>
<p><strong>假设2（CFL）</strong>：存在一个对参与方的分割$C=\{c1,…,c_K\},\cup _{k=1}^Kc_k=\{1,…,M\} $，在每一个分割$c \in C$内满足常规的联邦学习假设，即假设1。</p>
<h2 id="集群联邦学习"><a href="#集群联邦学习" class="headerlink" title="集群联邦学习"></a>集群联邦学习</h2><p>首先作者给出了该论文中涉及的重要数学符号的介绍，如下图。</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/2.jpg" class="">
<p>最重要的任务就是解决如何划分参与方。</p>
<h3 id="基于余弦相似度的二分法"><a href="#基于余弦相似度的二分法" class="headerlink" title="基于余弦相似度的二分法"></a>基于余弦相似度的二分法</h3><p><strong>定义1</strong>：当$M\ge K \ge 2$时，令$I:\{1,…,M\}\rightarrow\{1,…,K\}$为一个映射，将参与方$i$映射为数据分布的编号$I(i)$，对应的数据分布为$\phi_{I(i)}$，当且仅当下式成立时，称二分$c_1\cup c_2=\{1,…,M\}$是正确的（$c_1,c_2$均为非空集合）：</p>
<script type="math/tex; mode=display">I(i)\neq I(j) \qquad \forall i\in c_1,j\in c_2</script><p>简单来说，也就是当数据分布相同的参与方被分在了同一组时，划分就是正确的。</p>
<p>有了这个映射之后，就可以确定所有参与方本地数据的数据分布$D_i\sim\phi_{I(i)}(x,y)$，也可以确定每个参与方上的经验风险$r_i(\theta)=\sum_{(x,y)\in D_i}l_\theta (f(x),y)$，并且当参与方本地的数据集足够大时，有$r_i(\theta)\approx R_{I(i)}(\theta)$</p>
<p>假设有数据分布一共有两个，那么联邦学习的目标函数可以写作：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/3.png" class="">
<p>那么在最优解处，有<script type="math/tex">0=\nabla F(\theta^*)=a_1\nabla R_1(\theta^*)+a_2\nabla R_2(\theta^*)</script>，如果两个数据分布不全等，就会有<script type="math/tex">\nabla R_1(\theta^*)=-\frac{a_2}{a_1}\nabla R_2(\theta^*)\neq 0</script>，这说明<script type="math/tex">\nabla R_1(\theta^*)</script>和<script type="math/tex">\nabla R_2(\theta^*)</script>对应位置的元素异号（下图中的计算公式个人感觉将两个R都看作标量，那么计算余弦相似度时为什么一个是1一个是-1就好理解了）。故而，任意两个参与方的梯度更新的余弦相似度计算如下：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/4.png" class="">
<p>可以理解为在一个正确的划分下，随着联邦学习的推进，同一集群内部的梯度向量之间的余弦相似度会始终较小，但不同集群的梯度向量之间的余弦相似度会越来越多，如下图所示。</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/9.png" class="">
<p>随后就可以求取来自两个不同集群的参与方的梯度更新的最大余弦相似度，应让其最小化，并且有一个边界（边界推导过程没有看懂，回头再仔细看看原文吧……）：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/5.png" class="">
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/6.png" class="">
<p>这个很好理解，因为在一个合理的划分下，来自两个不同集群的样本之间余弦相似度应该很小，让两个集群中相似度最大的样本之间的相似度最小化，就得到了一个好的划分。同时还要求取来自同一个集群的任意两个样本的最小余弦相似度，这个最小余弦相似度也有边界（同样，推导过程回头再仔细看看原文，现在还没看懂……）：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/7.png" class="">
<p><strong>引理1</strong>：当$\alpha ^{min}_{intra}&gt;\alpha^{max}_{cross}$时，下式中的划分就是一个<strong>定义1</strong>中正确的划分：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/8.png" class="">
<p><strong>定义2</strong>：$g(\alpha):=\alpha _{intra}^{min}-\alpha^{max}_{cross}$，将其称之为组间差异，$g(\alpha)&gt;0$时，<strong>引理1</strong>中的划分就是对的。</p>
<h3 id="算法与解法"><a href="#算法与解法" class="headerlink" title="算法与解法"></a>算法与解法</h3><p>CFL算法首先将所有参与方看作同属一个集群$c=\{1,…,M\}$，进行联邦学习后得到的平稳解<script type="math/tex">\theta^*</script>（即使得联邦损失函数梯度绝对值小于<script type="math/tex">\epsilon_1</script>的解）如果满足$0\leq{\underset {i\in c}{max}}||\nabla_\theta r_i(\theta^*)||\leq \epsilon_2$，也就是在集群内的所有参与方上都将损失函数值降在了$\epsilon_2$以下，则CFL算法结束，得到目标模型；若无法满足，则说明需要进行集群切分。</p>
<h4 id="集群切分（二分）"><a href="#集群切分（二分）" class="headerlink" title="集群切分（二分）"></a>集群切分（二分）</h4><img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/10.png" class="">
<p>上图中的第3行，是将余弦相似度矩阵按值大小将索引排序（从大到小，有的博客说是从小到大，但我认为$argsort(-\alpha)$代表$\alpha$值从大到小），这里的索引是将二维矩阵压平，变成一维矩阵得到的索引。举个例子，如果$M=10$，那么（1，3）就转化为1*10+3=13。所以第6行中的$i_1$和$i_2$分别代表二维矩阵中的索引，即该值对应哪两个参与方之间的余弦相似度。这里就是每次把余弦相似度最大的两个参与方的分组合并，每一次有效合并都可以使$|C|$的大小降低，直至降低为2时终止，这样$C$中就有划分（二分）后的结果了。</p>
<h4 id="CFL算法"><a href="#CFL算法" class="headerlink" title="CFL算法"></a>CFL算法</h4><p>当切分后，如果不满足${\underset {i\in c}{max}}||\nabla_\theta r_i(\theta^*)||\leq \epsilon_2$，同时又满足$\alpha ^{min}_{intra}&gt;\alpha^{max}_{cross}$，那么就要继续切分。后者好计算，而前者较难计算，因此前者只能依据其定义式，进行估计如下：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/11.png" class="">
<p>因此，当$\gamma_{max}&lt;\sqrt{\frac{1-\alpha_{cross}^{max}}{2}}$时，划分就是正确的，并且可以继续划分。</p>
<p>所以CFL算法如下，这是一个递归过程，如果可以划分就继续划分，无须划分则返回目标模型：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/12.png" class="">
<p>该过程如下图所示：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/13.png" class="">
<h4 id="动态加入参与方"><a href="#动态加入参与方" class="headerlink" title="动态加入参与方"></a>动态加入参与方</h4><p>如果有参与方在训练中途加入，也是可以的。因为在每次进行划分时，CFL都会进行缓存划分两端的信息（如下图）：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/14.png" class="">
<p>中途加入的参与方只需按照下图算法就可以迅速找到自己所属的节点，并一同进行联邦学习：</p>
<img src="/2021/03/25/%E9%9B%86%E7%BE%A4%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%EF%BC%9AClustered-Federated-Learning/15.png" class=""> 
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文提出的CFL算法中虽然一些数学证明晦涩难懂，但是大体思路非常新颖，值得学习。</p>
]]></content>
      <categories>
        <category>联邦学习</category>
      </categories>
      <tags>
        <tag>联邦学习</tag>
      </tags>
  </entry>
</search>
